{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 3: How to Load, Convert, and Write JSON Files in Python\n",
    "## DS 6001: Practice and Application of Data Science\n",
    "\n",
    "### Instructions\n",
    "Please answer the following questions as completely as possible using text, code, and the results of code as needed. Format your answers in a Jupyter notebook. To receive full credit, make sure you address every part of the problem, and make sure your document is formatted in a clean and professional way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "Import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "sys.tracebacklimit = 0 # turn off the error tracebacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "JSON and CSV are both text-based formats for the storage of data. It's possible to open either one in a plain text editor. Given this similarity, why does a CSV file usually take less memory than a JSON formatted file for the same data? Under what conditions could a JSON file be smaller in memory than a CSV file for the same data? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">CSV files tend to be more compact, as they are pure text files in a tabular form. JSONs follow a \"tree\" or nested data structure that also allows for the inclusion of metadata.  \n",
    ">In most cases, a CSV will be smaller, but for huge datasets, a file with many, many missing values could potentially take up less memory in JSON form. Likewise, a large but unorganized dataset may be smaller in memory in JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "NASA has a dataset of all meteorites that have fallen to Earth between the years A.D. 860 and 2013. The data contain the name of each meteorite, along with the coordinates of the place where the meteorite hit, the mass of the meteorite, and the date of the collison. The data is stored as a JSON here: https://data.nasa.gov/resource/y77d-th95.json\n",
    "\n",
    "Look at the data in your web-browser and explain which strategy for loading the JSON into Python makes the most sense and why. \n",
    "\n",
    "Then write and run the code that will work for loading the data into Python. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The data is nested, so using the approach highlighted in 3.4.2 of *Surfing The Data Pipeline* is the easiest way to load this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>nametype</th>\n",
       "      <th>recclass</th>\n",
       "      <th>mass</th>\n",
       "      <th>fall</th>\n",
       "      <th>year</th>\n",
       "      <th>reclat</th>\n",
       "      <th>reclong</th>\n",
       "      <th>geolocation.type</th>\n",
       "      <th>geolocation.coordinates</th>\n",
       "      <th>:@computed_region_cbhk_fwbd</th>\n",
       "      <th>:@computed_region_nnqa_25f4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aachen</td>\n",
       "      <td>1</td>\n",
       "      <td>Valid</td>\n",
       "      <td>L5</td>\n",
       "      <td>21</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1880-01-01T00:00:00.000</td>\n",
       "      <td>50.775000</td>\n",
       "      <td>6.083330</td>\n",
       "      <td>Point</td>\n",
       "      <td>[6.08333, 50.775]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aarhus</td>\n",
       "      <td>2</td>\n",
       "      <td>Valid</td>\n",
       "      <td>H6</td>\n",
       "      <td>720</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1951-01-01T00:00:00.000</td>\n",
       "      <td>56.183330</td>\n",
       "      <td>10.233330</td>\n",
       "      <td>Point</td>\n",
       "      <td>[10.23333, 56.18333]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abee</td>\n",
       "      <td>6</td>\n",
       "      <td>Valid</td>\n",
       "      <td>EH4</td>\n",
       "      <td>107000</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1952-01-01T00:00:00.000</td>\n",
       "      <td>54.216670</td>\n",
       "      <td>-113.000000</td>\n",
       "      <td>Point</td>\n",
       "      <td>[-113, 54.21667]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acapulco</td>\n",
       "      <td>10</td>\n",
       "      <td>Valid</td>\n",
       "      <td>Acapulcoite</td>\n",
       "      <td>1914</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1976-01-01T00:00:00.000</td>\n",
       "      <td>16.883330</td>\n",
       "      <td>-99.900000</td>\n",
       "      <td>Point</td>\n",
       "      <td>[-99.9, 16.88333]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Achiras</td>\n",
       "      <td>370</td>\n",
       "      <td>Valid</td>\n",
       "      <td>L6</td>\n",
       "      <td>780</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1902-01-01T00:00:00.000</td>\n",
       "      <td>-33.166670</td>\n",
       "      <td>-64.950000</td>\n",
       "      <td>Point</td>\n",
       "      <td>[-64.95, -33.16667]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Tirupati</td>\n",
       "      <td>24009</td>\n",
       "      <td>Valid</td>\n",
       "      <td>H6</td>\n",
       "      <td>230</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1934-01-01T00:00:00.000</td>\n",
       "      <td>13.633330</td>\n",
       "      <td>79.416670</td>\n",
       "      <td>Point</td>\n",
       "      <td>[79.41667, 13.63333]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Tissint</td>\n",
       "      <td>54823</td>\n",
       "      <td>Valid</td>\n",
       "      <td>Martian (shergottite)</td>\n",
       "      <td>7000</td>\n",
       "      <td>Fell</td>\n",
       "      <td>2011-01-01T00:00:00.000</td>\n",
       "      <td>29.481950</td>\n",
       "      <td>-7.611230</td>\n",
       "      <td>Point</td>\n",
       "      <td>[-7.61123, 29.48195]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Tjabe</td>\n",
       "      <td>24011</td>\n",
       "      <td>Valid</td>\n",
       "      <td>H6</td>\n",
       "      <td>20000</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1869-01-01T00:00:00.000</td>\n",
       "      <td>-7.083330</td>\n",
       "      <td>111.533330</td>\n",
       "      <td>Point</td>\n",
       "      <td>[111.53333, -7.08333]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Tjerebon</td>\n",
       "      <td>24012</td>\n",
       "      <td>Valid</td>\n",
       "      <td>L5</td>\n",
       "      <td>16500</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1922-01-01T00:00:00.000</td>\n",
       "      <td>-6.666670</td>\n",
       "      <td>106.583330</td>\n",
       "      <td>Point</td>\n",
       "      <td>[106.58333, -6.66667]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Tomakovka</td>\n",
       "      <td>24019</td>\n",
       "      <td>Valid</td>\n",
       "      <td>LL6</td>\n",
       "      <td>600</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1905-01-01T00:00:00.000</td>\n",
       "      <td>47.850000</td>\n",
       "      <td>34.766670</td>\n",
       "      <td>Point</td>\n",
       "      <td>[34.76667, 47.85]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          name     id nametype               recclass    mass  fall  \\\n",
       "0       Aachen      1    Valid                     L5      21  Fell   \n",
       "1       Aarhus      2    Valid                     H6     720  Fell   \n",
       "2         Abee      6    Valid                    EH4  107000  Fell   \n",
       "3     Acapulco     10    Valid            Acapulcoite    1914  Fell   \n",
       "4      Achiras    370    Valid                     L6     780  Fell   \n",
       "..         ...    ...      ...                    ...     ...   ...   \n",
       "995   Tirupati  24009    Valid                     H6     230  Fell   \n",
       "996    Tissint  54823    Valid  Martian (shergottite)    7000  Fell   \n",
       "997      Tjabe  24011    Valid                     H6   20000  Fell   \n",
       "998   Tjerebon  24012    Valid                     L5   16500  Fell   \n",
       "999  Tomakovka  24019    Valid                    LL6     600  Fell   \n",
       "\n",
       "                        year      reclat      reclong geolocation.type  \\\n",
       "0    1880-01-01T00:00:00.000   50.775000     6.083330            Point   \n",
       "1    1951-01-01T00:00:00.000   56.183330    10.233330            Point   \n",
       "2    1952-01-01T00:00:00.000   54.216670  -113.000000            Point   \n",
       "3    1976-01-01T00:00:00.000   16.883330   -99.900000            Point   \n",
       "4    1902-01-01T00:00:00.000  -33.166670   -64.950000            Point   \n",
       "..                       ...         ...          ...              ...   \n",
       "995  1934-01-01T00:00:00.000   13.633330    79.416670            Point   \n",
       "996  2011-01-01T00:00:00.000   29.481950    -7.611230            Point   \n",
       "997  1869-01-01T00:00:00.000   -7.083330   111.533330            Point   \n",
       "998  1922-01-01T00:00:00.000   -6.666670   106.583330            Point   \n",
       "999  1905-01-01T00:00:00.000   47.850000    34.766670            Point   \n",
       "\n",
       "    geolocation.coordinates :@computed_region_cbhk_fwbd  \\\n",
       "0         [6.08333, 50.775]                         NaN   \n",
       "1      [10.23333, 56.18333]                         NaN   \n",
       "2          [-113, 54.21667]                         NaN   \n",
       "3         [-99.9, 16.88333]                         NaN   \n",
       "4       [-64.95, -33.16667]                         NaN   \n",
       "..                      ...                         ...   \n",
       "995    [79.41667, 13.63333]                         NaN   \n",
       "996    [-7.61123, 29.48195]                         NaN   \n",
       "997   [111.53333, -7.08333]                         NaN   \n",
       "998   [106.58333, -6.66667]                         NaN   \n",
       "999       [34.76667, 47.85]                         NaN   \n",
       "\n",
       "    :@computed_region_nnqa_25f4  \n",
       "0                           NaN  \n",
       "1                           NaN  \n",
       "2                           NaN  \n",
       "3                           NaN  \n",
       "4                           NaN  \n",
       "..                          ...  \n",
       "995                         NaN  \n",
       "996                         NaN  \n",
       "997                         NaN  \n",
       "998                         NaN  \n",
       "999                         NaN  \n",
       "\n",
       "[1000 rows x 13 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasa_data = requests.get(\"https://data.nasa.gov/resource/y77d-th95.json\")\n",
    "nasa_json = json.loads(nasa_data.text)\n",
    "nasa_df = pd.json_normalize(nasa_json)\n",
    "nasa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "The notebook for this module shows, as an example, how to pull data in JSON format from Reddit's top 25 posts on [/r/popular](https://www.reddit.com/r/popular/top/). The steps outlined there pull all of the features in the data into the dataframe, resulting in a dataframe with 172 columns. \n",
    "\n",
    "If we only wanted a few features, then looping across elements of the JSON list itself and extracting only the data we want may be a more efficient approach.\n",
    "\n",
    "Use looping - and not `pd.read_json()` or `pd.json_normalize()` - to create a dataframe with 25 rows (one for each of the top 25 posts), and only columns for `subreddit`, `title`, `ups`, and `created_utc`. The JSON file exists at http://www.reddit.com/r/popular/top.json, and don't forget to specify `headers = {'User-agent': 'DS6001'}` within `requests.get()`. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_reddit = \"http://www.reddit.com/r/popular/top.json\"\n",
    "reddit = requests.get(pop_reddit, headers = {'User-agent': 'DS6001'})\n",
    "reddit_json = json.loads(reddit.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>memes</td>\n",
       "      <td>Everytime</td>\n",
       "      <td>153266</td>\n",
       "      <td>1.600358e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dankmemes</td>\n",
       "      <td>College, man</td>\n",
       "      <td>142236</td>\n",
       "      <td>1.600353e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>memes</td>\n",
       "      <td>It do be like that</td>\n",
       "      <td>139111</td>\n",
       "      <td>1.600335e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nextfuckinglevel</td>\n",
       "      <td>This Turkish bride and groom chose their weddi...</td>\n",
       "      <td>132472</td>\n",
       "      <td>1.600345e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>funny</td>\n",
       "      <td>At least someone is having fun during quarentine</td>\n",
       "      <td>120590</td>\n",
       "      <td>1.600356e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit                                              title  \\\n",
       "0             memes                                          Everytime   \n",
       "1         dankmemes                                       College, man   \n",
       "2             memes                                 It do be like that   \n",
       "3  nextfuckinglevel  This Turkish bride and groom chose their weddi...   \n",
       "4             funny   At least someone is having fun during quarentine   \n",
       "\n",
       "      ups   created_utc  \n",
       "0  153266  1.600358e+09  \n",
       "1  142236  1.600353e+09  \n",
       "2  139111  1.600335e+09  \n",
       "3  132472  1.600345e+09  \n",
       "4  120590  1.600356e+09  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"subreddit\", \"title\", \"ups\", \"created_utc\"]\n",
    "new_reddit_list = [ [reddit_json[\"data\"][\"children\"][x][\"data\"][u] for u in columns  ] for x in range(25)]\n",
    "reddit_df = pd.DataFrame(new_reddit_list)\n",
    "reddit_df.columns = columns\n",
    "reddit_df.head(5) # preview the dataframe          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "The NBA has saved data on all 30 teams' shooting statistics for the 2014-2015 season here: https://stats.nba.com/js/data/sportvu/2015/shootingTeamData.json. Take a moment and look at this JSON file in your web browser. The structure of this particular JSON is complicated, but see if you can find the team-by-team data. In this problem our goal is to use `pd.json_normalize()` to get the data into a dataframe. The following questions will guide you towards this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "Download the raw text of the NBA JSON file and register it as JSON formatted data in Python's memory. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_url = \"https://stats.nba.com/js/data/sportvu/2015/shootingTeamData.json\"\n",
    "nba_data = requests.get(nba_url, headers = {'User-agent': 'DS6001'})\n",
    "nba_json = json.loads(nba_data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Describe, in words, the path that leads to the team-by-team data. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So, I went ahead and opened the file to determine if there was nesting and/or metadata. There's definitely nesting, so we need to access the \"rowSet\", which is an upper level column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Use the `pd.json_normalize()` function to pull the team-by-team data into a dataframe. This is going to be tricky. You will need to use indexing on the JSON data as well as the `record_path` parameter. \n",
    "\n",
    "If you are successful, you will have a dataframe with 30 rows and 33 columns. The first row will refer to the Golden State Warriors, the second row will refer to the San Antonio Spurs, and the third row will refer to the Cleveland Cavaliers. The columns will only be named 0, 1, 2, ... at this point. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1610612744</td>\n",
       "      <td>Golden State</td>\n",
       "      <td>Warriors</td>\n",
       "      <td>GSW</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.7</td>\n",
       "      <td>114.9</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478</td>\n",
       "      <td>21.2</td>\n",
       "      <td>42.5</td>\n",
       "      <td>0.497</td>\n",
       "      <td>2.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.363</td>\n",
       "      <td>10.8</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1610612759</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Spurs</td>\n",
       "      <td>SAS</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.3</td>\n",
       "      <td>103.5</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506</td>\n",
       "      <td>18.3</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.341</td>\n",
       "      <td>6.1</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1610612739</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>Cavaliers</td>\n",
       "      <td>CLE</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.7</td>\n",
       "      <td>104.3</td>\n",
       "      <td>16.9</td>\n",
       "      <td>0.481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473</td>\n",
       "      <td>18.2</td>\n",
       "      <td>40.7</td>\n",
       "      <td>0.447</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.299</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>0.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1610612746</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Clippers</td>\n",
       "      <td>LAC</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.6</td>\n",
       "      <td>104.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480</td>\n",
       "      <td>18.9</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.334</td>\n",
       "      <td>7.7</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0.373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1610612760</td>\n",
       "      <td>Oklahoma City</td>\n",
       "      <td>Thunder</td>\n",
       "      <td>OKC</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.6</td>\n",
       "      <td>110.2</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497</td>\n",
       "      <td>17.5</td>\n",
       "      <td>38.7</td>\n",
       "      <td>0.451</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.321</td>\n",
       "      <td>6.6</td>\n",
       "      <td>18.6</td>\n",
       "      <td>0.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1610612737</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Hawks</td>\n",
       "      <td>ATL</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.6</td>\n",
       "      <td>102.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483</td>\n",
       "      <td>19.4</td>\n",
       "      <td>44.6</td>\n",
       "      <td>0.435</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.311</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0              1          2    3  4   5     6      7     8      9   \\\n",
       "0  1610612744   Golden State   Warriors  GSW     82  48.7  114.9  14.9  0.498   \n",
       "1  1610612759    San Antonio      Spurs  SAS     82  48.3  103.5  14.8  0.481   \n",
       "2  1610612739      Cleveland  Cavaliers  CLE     82  48.7  104.3  16.9  0.481   \n",
       "3  1610612746    Los Angeles   Clippers  LAC     82  48.6  104.5  15.0  0.497   \n",
       "4  1610612760  Oklahoma City    Thunder  OKC     82  48.6  110.2  16.1  0.480   \n",
       "5  1610612737        Atlanta      Hawks  ATL     82  48.6  102.8  19.0  0.463   \n",
       "\n",
       "   ...     23    24    25     26   27   28     29    30    31     32  \n",
       "0  ...  0.478  21.2  42.5  0.497  2.3  6.3  0.363  10.8  25.3  0.429  \n",
       "1  ...  0.506  18.3  39.8  0.460  0.9  2.6  0.341   6.1  15.9  0.381  \n",
       "2  ...  0.473  18.2  40.7  0.447  1.7  5.7  0.299   9.0  23.9  0.378  \n",
       "3  ...  0.480  18.9  42.0  0.450  2.0  6.0  0.334   7.7  20.8  0.373  \n",
       "4  ...  0.497  17.5  38.7  0.451  1.6  5.1  0.321   6.6  18.6  0.356  \n",
       "5  ...  0.483  19.4  44.6  0.435  1.0  3.1  0.311   9.0  25.3  0.355  \n",
       "\n",
       "[6 rows x 33 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_url = \"https://stats.nba.com/js/data/sportvu/2015/shootingTeamData.json\"\n",
    "nba_data = requests.get(nba_url, headers = {'User-agent': 'DS6001'})\n",
    "nba_json = json.loads(nba_data.text)\n",
    "nba_df = pd.json_normalize(data = nba_json, record_path = [\"resultSets\", \"rowSet\"])\n",
    "nba_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "Find the path that leads to the headers (the column names), and extract these names as a list. Then set the `.columns` attribute of the dataframe you created in part c equal to this list. The result should be that the dataframe now has the correct column names. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM_ID</th>\n",
       "      <th>TEAM_CITY</th>\n",
       "      <th>TEAM_NAME</th>\n",
       "      <th>TEAM_ABBREVIATION</th>\n",
       "      <th>TEAM_CODE</th>\n",
       "      <th>GP</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PTS</th>\n",
       "      <th>PTS_DRIVE</th>\n",
       "      <th>FGP_DRIVE</th>\n",
       "      <th>...</th>\n",
       "      <th>CFGP</th>\n",
       "      <th>UFGM</th>\n",
       "      <th>UFGA</th>\n",
       "      <th>UFGP</th>\n",
       "      <th>CFG3M</th>\n",
       "      <th>CFG3A</th>\n",
       "      <th>CFG3P</th>\n",
       "      <th>UFG3M</th>\n",
       "      <th>UFG3A</th>\n",
       "      <th>UFG3P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1610612744</td>\n",
       "      <td>Golden State</td>\n",
       "      <td>Warriors</td>\n",
       "      <td>GSW</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.7</td>\n",
       "      <td>114.9</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478</td>\n",
       "      <td>21.2</td>\n",
       "      <td>42.5</td>\n",
       "      <td>0.497</td>\n",
       "      <td>2.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.363</td>\n",
       "      <td>10.8</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1610612759</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Spurs</td>\n",
       "      <td>SAS</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.3</td>\n",
       "      <td>103.5</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506</td>\n",
       "      <td>18.3</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.341</td>\n",
       "      <td>6.1</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1610612739</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>Cavaliers</td>\n",
       "      <td>CLE</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.7</td>\n",
       "      <td>104.3</td>\n",
       "      <td>16.9</td>\n",
       "      <td>0.481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473</td>\n",
       "      <td>18.2</td>\n",
       "      <td>40.7</td>\n",
       "      <td>0.447</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.299</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>0.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1610612746</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Clippers</td>\n",
       "      <td>LAC</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.6</td>\n",
       "      <td>104.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480</td>\n",
       "      <td>18.9</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.334</td>\n",
       "      <td>7.7</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0.373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1610612760</td>\n",
       "      <td>Oklahoma City</td>\n",
       "      <td>Thunder</td>\n",
       "      <td>OKC</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.6</td>\n",
       "      <td>110.2</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497</td>\n",
       "      <td>17.5</td>\n",
       "      <td>38.7</td>\n",
       "      <td>0.451</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.321</td>\n",
       "      <td>6.6</td>\n",
       "      <td>18.6</td>\n",
       "      <td>0.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1610612737</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Hawks</td>\n",
       "      <td>ATL</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.6</td>\n",
       "      <td>102.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483</td>\n",
       "      <td>19.4</td>\n",
       "      <td>44.6</td>\n",
       "      <td>0.435</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.311</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEAM_ID      TEAM_CITY  TEAM_NAME TEAM_ABBREVIATION TEAM_CODE  GP   MIN  \\\n",
       "0  1610612744   Golden State   Warriors               GSW            82  48.7   \n",
       "1  1610612759    San Antonio      Spurs               SAS            82  48.3   \n",
       "2  1610612739      Cleveland  Cavaliers               CLE            82  48.7   \n",
       "3  1610612746    Los Angeles   Clippers               LAC            82  48.6   \n",
       "4  1610612760  Oklahoma City    Thunder               OKC            82  48.6   \n",
       "5  1610612737        Atlanta      Hawks               ATL            82  48.6   \n",
       "\n",
       "     PTS  PTS_DRIVE  FGP_DRIVE  ...   CFGP  UFGM  UFGA   UFGP  CFG3M  CFG3A  \\\n",
       "0  114.9       14.9      0.498  ...  0.478  21.2  42.5  0.497    2.3    6.3   \n",
       "1  103.5       14.8      0.481  ...  0.506  18.3  39.8  0.460    0.9    2.6   \n",
       "2  104.3       16.9      0.481  ...  0.473  18.2  40.7  0.447    1.7    5.7   \n",
       "3  104.5       15.0      0.497  ...  0.480  18.9  42.0  0.450    2.0    6.0   \n",
       "4  110.2       16.1      0.480  ...  0.497  17.5  38.7  0.451    1.6    5.1   \n",
       "5  102.8       19.0      0.463  ...  0.483  19.4  44.6  0.435    1.0    3.1   \n",
       "\n",
       "   CFG3P  UFG3M  UFG3A  UFG3P  \n",
       "0  0.363   10.8   25.3  0.429  \n",
       "1  0.341    6.1   15.9  0.381  \n",
       "2  0.299    9.0   23.9  0.378  \n",
       "3  0.334    7.7   20.8  0.373  \n",
       "4  0.321    6.6   18.6  0.356  \n",
       "5  0.311    9.0   25.3  0.355  \n",
       "\n",
       "[6 rows x 33 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = nba_json[\"resultSets\"][0][\"headers\"]\n",
    "nba_df.columns = column_names\n",
    "nba_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "Save the NBA dataframe you extracted in problem 4 as a JSON-formatted text file on your local machine. Format the JSON so that it is organized as dictionary with three lists: `columns` lists the column names, `index` lists the row names, and `data` is a list-of-lists of data points, one list for each row. (Hint: this is possible with one line of code) (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.to_json(\"ShootyHoops.json\", orient = \"split\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
